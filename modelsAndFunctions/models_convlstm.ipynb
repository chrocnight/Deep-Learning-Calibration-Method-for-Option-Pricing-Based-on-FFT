{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{ConvLSTMCell}$定义了$\\textbf{ConvLSTM}$的基本单元(Cell)，实现了LSTM的核心逻辑--使用了卷积操作代替了全连接操作\n",
    "\n",
    "### 传统LSTM的门控机制\n",
    "$$\\mathbf{I}_{t}=\\sigma(\\mathbf{X}_{t}\\mathbf{W}_{xi}+\\mathbf{H}_{t-1}\\mathbf{W}_{hi}+\\mathbf{b}_{i})\n",
    "\\\\ \n",
    "\\mathbf{F}_{t}=\\sigma(\\mathbf{X}_{t}\\mathbf{W}_{xf}+\\mathbf{H}_{t-1}\\mathbf{W}_{hf}+\\mathbf{b}_{f}) \\\\\n",
    "\\mathbf{O}_{t}=\\sigma(\\mathbf{X}_{t}\\mathbf{W}_{xo}+\\mathbf{H}_{t-1}\\mathbf{W}_{ho}+\\mathbf{b}_{o}) \n",
    "\\\\\n",
    "\\tilde{\\mathbf{C}_{t}}=\\tanh(\\mathbf{X}_{t}\\mathbf{W}_{xc}+\\mathbf{H}_{t-1}\\mathbf{W}_{hc}+\\mathbf{b}_{c})(候选细胞状态更新) \\\\\n",
    "\\mathbf{C}_{t}=\\mathbf{F}_{t}\\odot \\mathbf{C}_{t-1}+\\mathbf{I}_{t}\\odot \\tilde{\\mathbf{C}_{t}}(细胞状态更新) \\\\\n",
    "\\mathbf{H}_{t}=\\mathbf{O}_{t}\\odot \\tanh(\\mathbf{C}_{t})\n",
    "\\\\\n",
    "\n",
    "\\tag{eq.1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改过的LSTM\n",
    "## 门的更新（一维卷积）\n",
    "$$\n",
    "i_{t} = \\sigma(W_{xi} * X_{t} + W_{hi}* H_{t-1} + W_{ci}\\odot C_{t-1}+b_{i}) \\\\\n",
    "f_{t} = \\sigma(W_{xf} * X_{t} + W_{hf}* H_{t-1} + W_{ci}\\odot C_{t-1}+b_{f}) \\\\\n",
    "C_{t} = f_{t} \\odot C_{t-1} + i_{t} \\odot \\tanh(W_{xc}*X_{t}+W_{hc} \\odot H_{t-1} + b_{c}) \\\\\n",
    "o_{t} = \\sigma(W_{xo}*X_{t} + W_{ho}* H_{t-1}+W_{co}*C_{t}+b_{o}) \\\\\n",
    "H_{t} = o_{t}\\odot \\tanh(C_{t}) \\\\\n",
    "\\tag{eq.1}\n",
    "$$\n",
    "## 主要是权重更新方法---调整bias项\n",
    "多出了$ W_{ci}$ $W_{cf}$ $W_{co}$---细胞状态的门控权重--用以调节细胞状态在门控机制中的影响\n",
    "这些权重产生的影响体现在：\n",
    "$$\n",
    "c_{i} = \\sigma(W_{xi}(x) + W_{hi}(h) + c\\odot W_{ci}) (输入门) \\\\\n",
    "c_{f} = \\sigma(W_{xf}(x) + W_{hf}(h) + c\\odot W_{cf}) (遗忘门) \\\\\n",
    "c_{o} = \\sigma(W_{xo}(x) + W_{ho}(h) + cc\\odot W_{co}) (输入门) \\\\\n",
    "\n",
    "\\tag{eq.2}\n",
    "$$\n",
    "\n",
    "以上 $eq.2$ 的基础上，ConvLSTM的前馈神经网络部分（mainly $forward()$） 中还包含了细胞状态更新和隐藏层更新的方程：\n",
    "$$\n",
    "c_{c} = \\stackrel{\\textcircled{1}}{c_f\\times c}  + \\stackrel{\\textcircled{2}}{c_{i} \\times \\tanh{(W_{xc}+W_{hc})}} \\\\\n",
    "c_{h} =  c_{0} \\times \\tanh{cc}\n",
    "\\tag{eq.3}\n",
    "$$\n",
    "\n",
    "$cc$--更新后的细胞状态\n",
    "$cf\\times c$--遗忘门的输出$c_{f}$与上一个时间步的细胞状态 $c$ 相乘，决定保留多少旧信息 \\\n",
    "$\\tanh(W_{xc}+W_{hc})$对输入$x$ 和隐状态$h$进行卷积操作，并通过$tanh$激活函数，得到候选细胞状态 \\\n",
    "$ \\textcircled{2}$: $c_{i}$与候选细胞状态相乘，决定添加多少新信息 \\\n",
    "\n",
    "$c_{o}\\times \\tanh(cc)$---更新后的细胞状态$cc$通过$\\tanh$函数，得到$\\textbf{候选隐状态}$,输出门$c_{o}$ \\\n",
    "与候选隐状态相乘，得到当前时间步的隐状态$c_{h}$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{只有隐状态会传递到输出层，而记忆元完全属于内部信息。}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上 $eq.1$ 中的卷积操作是一维卷积（而不是二维图形数据提取边缘信息的二维卷积矩阵操作）\n",
    "拿其中 $W_{xi}*X_{t}$表示如下一维卷积过程：\n",
    "-假设数据$X_{t}$ 是一个长度为5的1D向量： $X_{t}=[1,2,3,4,5]$ \n",
    "-卷积核$W_{xi}$是一个长度为3的向量： $W_{xi}=[1,0,-1]$\n",
    "$$\n",
    "W_{xi}*X_{t}=[1,0,-1]*[1,2,3,4,5] \\\\\n",
    "1\\times 1 + 2\\times 0+3\\times(-1) \\\\\n",
    "2\\times 1 + 3\\times 0 +4\\times(-1) \\\\\n",
    "3\\times 1+4\\times 0+5\\times (-1)\\\\\n",
    "=[-2,-2,-2] \\\\\n",
    "\\tag{eq.4}\n",
    "$$\n",
    "通过以上操作，卷积核 $W_{xi}$帮助模型从输入数据 $X_{t}$提取有用的信息特征（时间序列中的趋势、模式等），并将其整合到隐藏状态中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始输入数据的shape是 $(N,T,C,D)$，其中：\n",
    "-N: 样本数\n",
    "\n",
    "-T: 时间步\n",
    "\n",
    "-C: 通道数（3个）是\n",
    "\n",
    "    --fundamental data(spot price, strike price, day to expire, call or put, implied volatility)\n",
    "    --data of price (previous settle price, settle price change, theory price, theory magin(理论价差))\n",
    "    --data of greeks (Delta, Gamma, Theta, Vega, Rho)\n",
    "\n",
    "-D: 每个通道对应的5个特征\n",
    "\n",
    "为了做对比实验(多通道与单通道的对比)，作者将输入数据重新调整为 $(N,1,T,C \\times D)=(N,1,10,15)$\n",
    "\n",
    "    1:表示只有一个通道\n",
    "\n",
    "    $C\\times D=15$ ：将所有通道的特征拼在一起，即train_data.csv和test_data.csv的column变量都整合在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        # assert hidden_channels % 2 == 0\n",
    "        # 输入数据的通道数\n",
    "        self.input_channels = input_channels\n",
    "        # 隐藏状态的通道数\n",
    "        self.hidden_channels = hidden_channels\n",
    "        # kernel_size：卷积核的大小\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "        # W_{xi}}----输入门中输入X_{t}的权重\n",
    "        self.Wxi = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        # W_{hi}----输入门中 隐状态 H_{t-1} 的权重\n",
    "        self.Whi = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        # W_{xf}---遗忘门中 输入X_{t}的权重\n",
    "        self.Wxf = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        # W_{hf}---遗忘门中 隐状态 H_{t-1}的权重\n",
    "        self.Whf = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        # W_{xc}---输入 X_{t}的细胞状态\n",
    "        self.Wxc = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        # W_{hc}--- 隐状态 H_{t-1}的细胞状态\n",
    "        self.Whc = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        # 输出门的权重\n",
    "        self.Wxo = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        \n",
    "        self.Who = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "        \n",
    "    '''\n",
    "    forward(self, x, h, c)用于计算当前时间步的隐状态(hidden state)和细胞态(cell state)\n",
    "    input: 当前时间步的输入数据--x; 上一个时间步的隐状态--h;细胞状态--c\n",
    "    output: 当前时间步的隐状态 ch 和细胞状态 cc\n",
    "    '''\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "    # 为LSTM提供初始的隐状态和细胞状态\n",
    "    # batch_size--每次更新模型参数时所需的样本数量\n",
    "    # dataset一次训练不完，需要分多次小批次分批训练\n",
    "    # hidden--隐状态的通道数（即hidden_channels）\n",
    "    # dim--输入数据的维度\n",
    "    # nn.Parameter() 将张量包装为可训练的参数\n",
    "    # .cuda() 将参数移动到CUDA上(如果GPU可用)\n",
    "    def init_hidden(self, batch_size, hidden, dim):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = nn.Parameter(torch.zeros(1, hidden, dim)).cuda()\n",
    "            self.Wcf = nn.Parameter(torch.zeros(1, hidden, dim)).cuda()\n",
    "            self.Wco = nn.Parameter(torch.zeros(1, hidden, dim)).cuda()\n",
    "        else:\n",
    "            # 如果self.Wci已经初始化 检查输入数据的维度 dim是否与self.Wci第三维度匹配\n",
    "            assert dim == self.Wci.size()[2], 'Input Dim Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, dim)).cuda(),\n",
    "                Variable(torch.zeros(batch_size, hidden, dim)).cuda())\n",
    "        # 返回的两个对象-h和c--初始化的隐状态+细胞状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, in_dim, out_dim, step=10):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        # 将输入通道数和隐藏通道数合并为同一个列表，表示为每一层的输入通道数\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        # LSTM的层数 等于hidden_channels的长度  \n",
    "        self.num_layers = len(hidden_channels)\n",
    "        # 时间步数，表示输入序列的长度\n",
    "        self.step = step\n",
    "        self._all_layers = []\n",
    "        # 定义一个全连接层，用于将LSTM的输出映射到目标维度--最后一个得到optionprice的步骤\n",
    "        self.linear = nn.Linear(in_features=in_dim, out_features=out_dim)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # 每一个层生成一个唯一的名称， 例如cell0, cell1\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            ## 将ConvLSTMCell实例动态添加到 ConvLSTM类中，使其成为类的属性\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "# N--观察数\n",
    "# C--通道数channels\n",
    "# D--变量(Delta Gamma Theta Vega Rho)\n",
    "    def forward(self, input):\n",
    "        # input:(N, C, T, D_in)\n",
    "        # N--batch size---一次处理的数据样本数量\n",
    "        # C--channels--每个时间步的输入特征数\n",
    "        # T--time steps--时间序列的长度\n",
    "        # D_in --- input dimension---每个时间步的特征向量的维度\n",
    "        # internel_state-用于存储每一层的LSTM单元的隐藏状态(h,c) 其中 h--hidden state, c--cell state\n",
    "        internal_state = []\n",
    "        '''\n",
    "        假设 \n",
    "        N=2----2个样本\n",
    "        C=3----3个通道\n",
    "        T=3----3个时间步\n",
    "        D_in=2-每个时间步的特征向量有2维\n",
    "        input = torch.tensor([\n",
    "    # 样本 1\n",
    "    [\n",
    "        # 通道 1\n",
    "        [[1.1, 1.2], [2.1, 2.2], [3.1, 3.2]],\n",
    "        # 通道 2\n",
    "        [[1.3, 1.4], [2.3, 2.4], [3.3, 3.4]],\n",
    "        # 通道 3\n",
    "        [[1.5, 1.6], [2.5, 2.6], [3.5, 3.6]]\n",
    "    ],\n",
    "    # 样本 2\n",
    "    [\n",
    "        # 通道 1\n",
    "        [[4.1, 4.2], [5.1, 5.2], [6.1, 6.2]],\n",
    "        # 通道 2\n",
    "        [[4.3, 4.4], [5.3, 5.4], [6.3, 6.4]],\n",
    "        # 通道 3\n",
    "        [[4.5, 4.6], [5.5, 5.6], [6.5, 6.6]]\n",
    "    ]\n",
    "])\n",
    "        \n",
    "        '''\n",
    "        # self.step = 10 是是个样本\n",
    "        for step in range(self.step):\n",
    "            x = input[:, :, step, :]\n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                if step == 0:\n",
    "                    bsize, _, dim = x.size()\n",
    "                    # 获取第 i 层的LSTM单元\n",
    "                    # 调用LSTM单元初始化方法，返回初始化的隐藏状态（h,c）\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                             dim=dim)\n",
    "                    internal_state.append((h, c))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            outputs = x\n",
    "            # outputs:(N, hidden_channels[-1], D_in)\n",
    "            outputs = self.linear(outputs)\n",
    "            # outputs:(N, hidden_channels[-1], D_out)\n",
    "\n",
    "        return outputs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "if __name__ == '__main__':\n",
    "    input = torch.normal(0, 1, size=(64, 3, 10, 5)).cuda()\n",
    "    model = ConvLSTM(input_channels=3, hidden_channels=[3, 1], kernel_size=3, in_dim=5, out_dim=1, step=10).cuda()\n",
    "    output = model.forward(input)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设\n",
    "\n",
    "`input = torch.normal(0, 1, size=(64, 3, 10, 5)).cuda()` \n",
    "\n",
    "+ N=64--batch size\n",
    "+ C=3--channels\n",
    "+ T=10--time steps(same option in past 10 days)\n",
    "+ D_in=5--5 dimensions in every timestep(fundamental data, data of price, data of greeks) \n",
    "\n",
    "model initalization:\n",
    "`model = ConvLSTM(input_channels=3, hidden_channels=[3,1], kernel_size=3, in_dim=5, out_dim=1, step=10).cuda()`\n",
    "- `hidden_channels=[3,1]`: two layers of LSTM units, hidden channels of layer 1 is 3, layer 2 is 1\n",
    "\n",
    "### Running process\n",
    "1. model initializaiton will initialize two layers of LSTM units:\n",
    "- 1st layer: `ConvLSTMCell(input_channels=3, hidden_channels=3, kernel_size=3)`\n",
    "- 2nd layer: `ConvLSTMCell(input_channels=3, hidden_channels=1, kernel_size=3)`\n",
    "\n",
    "2. forward\n",
    "output = model.forward(input)\n",
    "dimention of `input` is `(64, 3, 10, 5)`\n",
    "\n",
    "    2.1 loops by time step\n",
    "    `x=input[:, :, step ,:]`\n",
    "    - dimension of x is `(64, 3, 5)`--represents features of  all samples and channels of current time step\n",
    "    2.2 loops by layer\n",
    "    for every layer i (0 to 1):\n",
    "    - initialize state  (h,c) in case of `step==0`:\n",
    "    `(h,c)=getattr(self, name).init_hidden(batch_size=64, hidden=self.hidden_channels[i], dim=5)`\n",
    "        - 1st layer : dimension of `h` and `c` is (64, 3, 5)\n",
    "        - 2nd layer : dimension of `h` and `c` is (64, 1, 5) \n",
    "    - do forward of LSTM\n",
    "    `x, new_c=getattr(self, name)(x, h, c) `\n",
    "    - update hidden sate:\n",
    "    `internal_state[i]=(x, new_c)`\n",
    "\n",
    "    - process output\n",
    "    at the last time `step=9` , `x` is the output of layer 2, dimension is (64, 1, 5)\n",
    "    we can map the output to the target dimension through a linear layer:\n",
    "    `outputs = self.linear(outputs)` dimension of `outputs` is  `(64, 1, 1)`\n",
    "    - return results\n",
    "    `return outputs.squeeze()`\n",
    "    dimension of outputs is (64,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
